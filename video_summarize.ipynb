{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17gkk-5v6E3uYZq0n9TOzUcbMVogNUiIM",
      "authorship_tag": "ABX9TyNSZdHHT6OK3xLNPi/ALmzI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandra-prakash-94/GenAI/blob/main/video_summarize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Extract required number of still images(frames) from a given video"
      ],
      "metadata": {
        "id": "sba7zT45tjS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "fePo60YI2Kwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4PJj1G28aHV"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of frames to extract\n",
        "num_frames = 10\n",
        "\n",
        "# Open video file\n",
        "vidcap = cv2.VideoCapture('/content/drive/MyDrive/Colab Notebooks/Video_summary/theft_sample2.mp4')\n"
      ],
      "metadata": {
        "id": "dxvLBl2h-ZSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get video properties\n",
        "fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
      ],
      "metadata": {
        "id": "Bsmov5YkzegR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU97BjFjzjUC",
        "outputId": "962b6f8a-95f9-4b5f-f834-b384a093ae17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total number of frames present in the input video\n",
        "frame = 0\n",
        "while vidcap.isOpened():\n",
        "    success, image = vidcap.read()\n",
        "    if success:\n",
        "      frame += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "total_frames = frame\n",
        "vidcap.release()"
      ],
      "metadata": {
        "id": "CpNS7QPWzKRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_frames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUW1b4DGzu6B",
        "outputId": "500a427d-5d34-4503-e9bc-0f4831e513e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "338"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate frame interval\n",
        "frame_interval = int(total_frames/num_frames)"
      ],
      "metadata": {
        "id": "l2ZhqSMy22wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_interval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmvNHEkd0bLs",
        "outputId": "b93951d0-d56b-40a9-d122-f4a46c33bc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract frame for every frame interval"
      ],
      "metadata": {
        "id": "fcon_uX-uKuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidcap = cv2.VideoCapture('/content/drive/MyDrive/Colab Notebooks/Video_summary/theft_sample2.mp4')"
      ],
      "metadata": {
        "id": "6tD1lU2S7G2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize frame counter\n",
        "count = 0\n",
        "\n",
        "# Loop through frames\n",
        "while vidcap.isOpened():\n",
        "    success, image = vidcap.read()\n",
        "    if success:\n",
        "      if(count%frame_interval==0):\n",
        "          cv2.imwrite(os.path.join('/content/drive/MyDrive/Colab Notebooks/Video_summary/images_extracted1/', 'frame%d.jpg' %count), image)\n",
        "          print('Frame {} saved'.format(count))\n",
        "      count += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "vidcap.release()"
      ],
      "metadata": {
        "id": "rY8QgkOxp-wh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86f7227-1b00-4507-e5e1-a7c42ef3d143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 0 saved\n",
            "Frame 33 saved\n",
            "Frame 66 saved\n",
            "Frame 99 saved\n",
            "Frame 132 saved\n",
            "Frame 165 saved\n",
            "Frame 198 saved\n",
            "Frame 231 saved\n",
            "Frame 264 saved\n",
            "Frame 297 saved\n",
            "Frame 330 saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. List out the file names of extracted images and make a combined file name list"
      ],
      "metadata": {
        "id": "tAPXbem4ukqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_files_and_make_list(path):\n",
        "    # List out files from the given path\n",
        "    files = os.listdir(path)\n",
        "\n",
        "    # Create a list of file names\n",
        "    file_names = []\n",
        "    for file in files:\n",
        "          file_names.append(file)\n",
        "\n",
        "    return file_names\n",
        "\n",
        "# usage:\n",
        "directory_path = \"/content/drive/MyDrive/Colab Notebooks/Video_summary/images_extracted1\"\n",
        "file_names = list_files_and_make_list(directory_path)\n"
      ],
      "metadata": {
        "id": "ISgFpNsXmgCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWm-xQZ2my-m",
        "outputId": "c9cd2771-b04a-4972-aa01-1d00bad53de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['frame0.jpg',\n",
              " 'frame33.jpg',\n",
              " 'frame66.jpg',\n",
              " 'frame99.jpg',\n",
              " 'frame132.jpg',\n",
              " 'frame165.jpg',\n",
              " 'frame198.jpg',\n",
              " 'frame231.jpg',\n",
              " 'frame264.jpg',\n",
              " 'frame297.jpg',\n",
              " 'frame330.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Generate caption for each image present in combined list of images using transformers 'image-to-text' model and make a list having captions from each of the images"
      ],
      "metadata": {
        "id": "GnLXO67pvIQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "image_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n"
      ],
      "metadata": {
        "id": "yVTXrovY-lfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_captions_for_images(file_names):\n",
        "    captions = []\n",
        "    for image in file_names:\n",
        "        caption = image_to_text(\"/content/drive/MyDrive/Colab Notebooks/Video_summary/images_extracted1/{}\".format(image))\n",
        "        captions.append(caption[0]['generated_text'])\n",
        "    return captions\n"
      ],
      "metadata": {
        "id": "4zYrvevhnzxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# usage:\n",
        "captions = generate_captions_for_images(file_names)\n",
        "captions"
      ],
      "metadata": {
        "id": "Y3Ah_rjm_YD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f90933-55da-4157-e597-d29cf78cdfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a blue and black skateboard sitting on a tile floor ',\n",
              " 'a blue and black skateboard sitting on a tile floor ',\n",
              " 'a person standing in a room with a blue and white surfboard ',\n",
              " 'a person standing in a room with a suitcase ',\n",
              " 'a man is standing in front of a blue and white kite ',\n",
              " 'a man is holding a small blue and white kite ',\n",
              " 'a man is standing in a room with a blue and white t-shirt ',\n",
              " 'a man is using a pair of scissors to cut a skateboard ',\n",
              " 'a man holding a cell phone in front of a mirror ',\n",
              " 'a person standing in a room with a suitcase ',\n",
              " 'a person is holding a blue and white polka dot polka dot umbrella ']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Make a combined paragraph from captions generated and summarize the para using transformers 'summarization' model\n"
      ],
      "metadata": {
        "id": "38b8G7SKrFym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n"
      ],
      "metadata": {
        "id": "h-EXrn5-rJrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_texts(captions):\n",
        "    # Join the list of texts into a single paragraph\n",
        "    combined_paragraph = \" \".join(captions)\n",
        "    return combined_paragraph\n"
      ],
      "metadata": {
        "id": "0blUMoImslzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# usage:\n",
        "combined_paragraph = combine_texts(captions)\n",
        "combined_paragraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "hwpgzIMQsrpP",
        "outputId": "616c8bb6-d1eb-4ffb-bf33-81671836788e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a blue and black skateboard sitting on a tile floor  a blue and black skateboard sitting on a tile floor  a person standing in a room with a blue and white surfboard  a person standing in a room with a suitcase  a man is standing in front of a blue and white kite  a man is holding a small blue and white kite  a man is standing in a room with a blue and white t-shirt  a man is using a pair of scissors to cut a skateboard  a man holding a cell phone in front of a mirror  a person standing in a room with a suitcase  a person is holding a blue and white polka dot polka dot umbrella '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ARTICLE = combined_paragraph\n",
        "print(summarizer(ARTICLE, max_length=50, min_length=30, do_sample=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qU0mE90ruZz",
        "outputId": "17841b4a-16eb-4c60-a381-128234488805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'A man is using a pair of scissors to cut a skateboard. A person is standing in a room with a suitcase. A man is holding a small blue and white kite.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JvOUp8wUtFrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}